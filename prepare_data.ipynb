{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "adverse-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "square-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_path = os.path.join('datasets', 'movie_conversations.txt')\n",
    "convs = []\n",
    "with open(conv_path) as file:\n",
    "    for row in file:\n",
    "        one_line = row.split('+++$+++')[-1]\n",
    "        one_line = one_line.strip()\n",
    "        one_line = re.sub('[\\[\\'\\]]', '', one_line)\n",
    "        convs.append(one_line.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "controlled-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "convs_pairs = []\n",
    "for row in convs:\n",
    "    while len(row) > 1:\n",
    "        convs_pairs.append(row[:2])\n",
    "        row = row[1:]\n",
    "convs_pairs = pd.DataFrame(convs_pairs, \n",
    "                           columns=['question_code', 'answer_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "smoking-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_path = os.path.join('datasets', 'movie_lines.txt')\n",
    "lines_convs = []\n",
    "with open(lines_path) as file:\n",
    "    for row in file:\n",
    "        one_line = row.split('+++$+++')\n",
    "        code, text = one_line[0], one_line[-1]\n",
    "        lines_convs.append([code.strip(), text.strip()])\n",
    "code_lines = pd.DataFrame(lines_convs, columns=['code', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "geological-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.merge(convs_pairs, code_lines, \n",
    "                   left_on='question_code', right_on='code',\n",
    "                   how='left')\n",
    "text_df.rename(columns={'text': 'question'},\n",
    "               inplace=True)\n",
    "text_df = pd.merge(text_df, code_lines, \n",
    "                   left_on='answer_code', right_on='code',\n",
    "                   how='left')\n",
    "text_df.rename(columns={'text': 'answer'},\n",
    "               inplace=True)\n",
    "text_df.drop(columns=['code_x', 'code_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "meaning-disease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_code</th>\n",
       "      <th>answer_code</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L194</td>\n",
       "      <td>L195</td>\n",
       "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L195</td>\n",
       "      <td>L196</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L196</td>\n",
       "      <td>L197</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L198</td>\n",
       "      <td>L199</td>\n",
       "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
       "      <td>Forget it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L200</td>\n",
       "      <td>L201</td>\n",
       "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
       "      <td>Cameron.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_code answer_code  \\\n",
       "0          L194        L195   \n",
       "1          L195        L196   \n",
       "2          L196        L197   \n",
       "3          L198        L199   \n",
       "4          L200        L201   \n",
       "\n",
       "                                            question  \\\n",
       "0  Can we make this quick?  Roxanne Korrine and A...   \n",
       "1  Well, I thought we'd start with pronunciation,...   \n",
       "2  Not the hacking and gagging and spitting part....   \n",
       "3  You're asking me out.  That's so cute. What's ...   \n",
       "4  No, no, it's my fault -- we didn't have a prop...   \n",
       "\n",
       "                                              answer  \n",
       "0  Well, I thought we'd start with pronunciation,...  \n",
       "1  Not the hacking and gagging and spitting part....  \n",
       "2  Okay... then how 'bout we try out some French ...  \n",
       "3                                         Forget it.  \n",
       "4                                           Cameron.  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-opening",
   "metadata": {},
   "source": [
    "Check out [that](https://gist.github.com/MrEliptik/b3f16179aa2f530781ef8ca9a16499af)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "accomplished-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "def remove_punctuations(words):\n",
    "    clean_tokens = [word for word in words if word not in string.punctuation]\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def replace_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "    \n",
    "\n",
    "def clean_text(text):\n",
    "    text = make_lowercase(text)\n",
    "    text = remove_urls(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_punctuations(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['quest_tokens'] = text_df['question'].apply(clean_text)\n",
    "text_df['answer_tokens'] = text_df['answer'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-disorder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
