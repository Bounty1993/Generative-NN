{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "placed-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Conv2DTranspose, Reshape, BatchNormalization\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "hybrid-manchester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'autoencoder.ipynb',\n",
       " 'datasets',\n",
       " 'full_numpy_bitmap_angel.npy',\n",
       " 'GANs.ipynb',\n",
       " 'input_data.csv',\n",
       " 'prepare_data.ipynb',\n",
       " 'seq2seq_model.ipynb',\n",
       " 'text_summary.ipynb']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "daily-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load('full_numpy_bitmap_angel.npy')\n",
    "x_train = []\n",
    "for x in t:\n",
    "    x_train.append(x.reshape(28, 28, 1))\n",
    "x_train = np.array(x_train)\n",
    "x_train = np.float32(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "lonely-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Discriminator, self).__init__(**kwargs)\n",
    "        self.dense_1 = Dense(64, activation='relu')\n",
    "        self.dense_2 = Dense(32, activation='relu')\n",
    "        self.dense_3 = Dense(32, activation='relu')\n",
    "        self.flat_layer = Flatten()\n",
    "        self.output_layer = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.flat_layer(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Generator(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Generator, self).__init__(**kwargs)\n",
    "        self.dense_1 = Dense(7*7*256, activation='relu', input_shape=(100,), name='dense_1')\n",
    "        self.reshape = Reshape((7, 7, 256), name='reshape')\n",
    "        \n",
    "        self.conv2dt_1 = Conv2DTranspose(128, (3, 3), strides=1, padding='same', name='conv2dt_1')\n",
    "        self.bn1 = BatchNormalization(name='bn1')\n",
    "        \n",
    "        self.conv2dt_2 = Conv2DTranspose(64, (3, 3), strides=2, padding='same', name='conv2dt_2')\n",
    "        self.bn2 = BatchNormalization(name='bn2')\n",
    "        \n",
    "        self.conv2dt_3 = Conv2DTranspose(1, (3, 3), strides=2, padding='same', \n",
    "                                         activation='tanh', name='conv2dt_3')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.reshape(x)\n",
    "        x = self.conv2dt_1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2dt_2(x)\n",
    "        x = self.bn2(x)\n",
    "        output = self.conv2dt_3(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class GAN(Model):\n",
    "    def __init__(self, latent_dim, **kwargs):\n",
    "        super(GAN, self).__init__(**kwargs)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.generator = Generator()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        fake_img = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        fake_img = self.generator(fake_img)\n",
    "        both_img = tf.concat([fake_img, data], axis=0)\n",
    "        labels = tf.concat([tf.ones((batch_size, 1)),\n",
    "                            tf.zeros((batch_size, 1))],\n",
    "                            axis=0)\n",
    "        labels += 0.05 * tf.random.uniform(shape=(2 * batch_size, 1))\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.discriminator(both_img)\n",
    "            d_loss = self.loss_fn(labels, preds)\n",
    "            \n",
    "        grad = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(zip(grad, self.discriminator.trainable_weights))\n",
    "        \n",
    "        random_input = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        gen_labels = tf.zeros((batch_size, 1))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.discriminator(self.generator(random_input))\n",
    "            g_loss = self.loss_fn(gen_labels, preds)\n",
    "            \n",
    "        grad = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grad, self.generator.trainable_weights))\n",
    "\n",
    "        return {'d_loss': d_loss, 'g_loss': g_loss}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ordered-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "y_train = y_train\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "union-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx_train = np.where(y_train == 1)[0]\n",
    "#x_train = x_train[idx_train]\n",
    "\n",
    "#idx_test = np.where(y_test == 1)[0]\n",
    "#x_test = x_test[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "pediatric-germany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4680/4680 [==============================] - 588s 126ms/step - d_loss: 0.4911 - g_loss: 1.3122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1359057f850>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(labels, preds):\n",
    "    return losses.BinaryCrossentropy(from_logits=True)(labels, preds)\n",
    "\n",
    "d_optimizer = optimizers.Adam(lr=0.001)\n",
    "g_optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model = GAN(latent_dim=100)\n",
    "\n",
    "model.compile(d_optimizer, g_optimizer, loss_fn)\n",
    "model.fit(x_train, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
