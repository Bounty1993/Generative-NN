{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "unknown-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "joint-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2                  Why the Truth Might Get You Fired   \n",
       "3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4  Iranian woman jailed for fictional unpublished...   \n",
       "\n",
       "                                                text  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...  \n",
       "1  Ever get the feeling your life circles the rou...  \n",
       "2  Why the Truth Might Get You Fired October 29, ...  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = os.path.join('datasets', 'summary_train.csv')\n",
    "train_df = pd.read_csv(train_path, usecols=['title', 'text'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "parliamentary-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "text = train_df['text'].values\n",
    "targets = train_df['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "copyrighted-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TARGETS_LENGTH = 20\n",
    "\n",
    "def preprocess_targets(targets):\n",
    "    seq_list = []\n",
    "    for line in targets:\n",
    "        words_list = []\n",
    "        sentence = '[bos] ' + line.lower() + ' [eos]'\n",
    "        sentence = sentence.split(' ')\n",
    "        for word in sentence:\n",
    "            if len(words_list) < MAX_TARGETS_LENGTH:\n",
    "                words_list.append(word)\n",
    "            else:\n",
    "                break\n",
    "        seq_list.append(words_list)\n",
    "    return seq_list\n",
    "            \n",
    "targets = preprocess_targets(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "spoken-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10_000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(text)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "scheduled-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {word: index for word, index in word_index.items() if index <= 10_000}\n",
    "index_word = {index: word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "based-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "new_text = tokenizer.texts_to_sequences(text)\n",
    "new_text = pad_sequences(new_text, maxlen=MAX_LEN, \n",
    "                         truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "therapeutic-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_mapping(text):\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    word_index = {word: index for word, index in word_index.items() if index <= 10_000}\n",
    "    index_word = {index: word for word, index in word_index.items()}\n",
    "    return word_index, index_word\n",
    "\n",
    "\n",
    "\n",
    "target_word_index, target_index_word = get_words_mapping(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "painful-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_MAX_LEN = 100\n",
    "\n",
    "def get_generator(new_text, targets, batch_size=124):\n",
    "    nrows = new_text.shape[0]\n",
    "    num_batches = nrows // batch_size\n",
    "    for batch_id in range(num_batches):\n",
    "        low = batch_id * batch_size\n",
    "        upper = (batch_id + 1) * batch_size\n",
    "        encoder_input_data = new_text[low:upper]\n",
    "        decoder_output_data = np.zeros(shape=(batch_size, TARGET_MAX_LEN, NUM_WORDS + 1))\n",
    "        decoder_input_data = np.zeros(shape=(batch_size, TARGET_MAX_LEN, NUM_WORDS + 1))\n",
    "        for i, line in enumerate(targets[low:upper]):\n",
    "            for j, word in enumerate(line):\n",
    "                if j >= TARGET_MAX_LEN:\n",
    "                    break\n",
    "                try:\n",
    "                    idx = target_word_index[word]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                else:\n",
    "                    decoder_input_data[i, j, idx] = 1\n",
    "                    if i > 0:\n",
    "                        decoder_output_data[i, j-1, idx] = 1\n",
    "        yield [encoder_input_data, decoder_input_data], decoder_output_data\n",
    "        \n",
    "[encoder_input_data, decoder_input_data], decoder_output_data = get_generator(new_text, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "speaking-burton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 100)    1000000     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None, 10000) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, None, 64), ( 42240       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 64), ( 2576640     decoder_input[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 10000)  650000      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,268,880\n",
      "Trainable params: 4,268,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=(None,), name='encoder_input')\n",
    "encoder_embedding = Embedding(input_dim=NUM_WORDS, output_dim=100, \n",
    "                              input_length=MAX_LEN, name='encoder_embedding')\n",
    "encoder_lstm = LSTM(64, return_state=True, \n",
    "                    return_sequences=True, name='encoder_lstm')\n",
    "encoder_output, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_input))\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = Input(shape=(None, NUM_WORDS), name='decoder_input')\n",
    "decoder_lstm = LSTM(64, return_state=True, \n",
    "                    return_sequences=True, name='decoder_lstm')\n",
    "decoder_output, decoder_state_h, decoder_state_c = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=NUM_WORDS, activation='softmax', name='decoder_dense')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-latter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
