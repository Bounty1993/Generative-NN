{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "hired-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Layer, Input, Conv2D, BatchNormalization, Dropout, Dense, \n",
    "                                     Flatten, Reshape, Conv2DTranspose)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "handy-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, num_conv_layers, encoder_conv_filters, encoder_strides,\n",
    "                 encoder_output_dim, dropout_rate=None, input_dim=None, name='encoder', **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.conv_layers = []\n",
    "        for i in range(num_conv_layers):\n",
    "            conv_layer = Conv2D(filters=encoder_conv_filters[i],\n",
    "                                kernel_size=(3, 3),\n",
    "                                strides=encoder_strides[i],\n",
    "                                padding='same',\n",
    "                                activation='relu',\n",
    "                                name=f'encoder_conv_{i}')\n",
    "            self.conv_layers.append(conv_layer)\n",
    "    \n",
    "        self.dense_layer = Dense(encoder_output_dim, name='encoder_output')\n",
    "        \n",
    "    \n",
    "    def model(self):\n",
    "        model_input = Input(shape=self.input_dim)\n",
    "        return Model(model_input, self.call(model_input))\n",
    "            \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        x = Flatten()(x)\n",
    "        self.shape_before_flat = K.int_shape(x)[1:] \n",
    "        x = self.dense_layer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Decoder(Layer):\n",
    "    def __init__(self, num_conv_layers, decoder_conv_filters, shape_before_flat, \n",
    "                 decoder_strides, dropout_rate=None, input_dim=None, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.dense = Dense(np.prod(shape_before_flat), activation='relu')\n",
    "        self.reshape = Reshape(shape_before_flat)\n",
    "        self.conv_layers = []\n",
    "        for i in range(num_conv_layers - 1):\n",
    "            conv_layer = Conv2DTranspose(filters=decoder_conv_filters[i],\n",
    "                                         kernel_size=(3, 3),\n",
    "                                         strides=decoder_strides[i],\n",
    "                                         padding='same',\n",
    "                                         activation='relu',\n",
    "                                         name=f'decoder_conv_{i}')\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            \n",
    "        self.output_layer = Conv2DTranspose(filters=1,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=decoder_strides[-1],\n",
    "                                            padding='same',\n",
    "                                            activation='sigmoid', \n",
    "                                            name=f'output_layer')\n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        model_input = Input(shape=self.input_dim)\n",
    "        return Model(model_input, self.call(model_input))\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        x = self.reshape(x)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    \n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, \n",
    "                 num_conv_layers,\n",
    "                 encoder_conv_filters,\n",
    "                 encoder_strides,\n",
    "                 encoder_output_dim, \n",
    "                 input_dim, \n",
    "                 decoder_conv_filters, \n",
    "                 shape_before_flat, \n",
    "                 decoder_strides,\n",
    "                 dropout_rate=None, \n",
    "                 **kwargs):\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        self.encoder = Encoder(num_conv_layers = num_conv_layers, \n",
    "                               encoder_conv_filters = encoder_conv_filters,\n",
    "                               encoder_strides = encoder_strides,\n",
    "                               encoder_output_dim = encoder_output_dim, \n",
    "                               input_dim = input_dim,\n",
    "                               dropout_rate = dropout_rate)\n",
    "        self.decoder = Decoder(num_conv_layers = num_conv_layers, \n",
    "                               decoder_conv_filters = decoder_conv_filters, \n",
    "                               shape_before_flat = shape_before_flat, \n",
    "                               decoder_strides = decoder_strides,\n",
    "                               input_dim=(2,),\n",
    "                               dropout_rate=dropout_rate)\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        encoder_output = self.encoder(inputs)\n",
    "        decoder_output = self.decoder(encoder_output)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "welsh-fight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)      (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)      (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)      (None, 7, 7, 32)          18464     \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "encoder_output (Dense)       (None, 2)                 3138      \n",
      "=================================================================\n",
      "Total params: 77,346\n",
      "Trainable params: 77,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_conv_layers = 4\n",
    "encoder_conv_filters = [32, 64, 64, 32]\n",
    "encoder_output_dim=2\n",
    "encoder_strides = [1,2,2,1]\n",
    "input_dim=(28, 28, 1)\n",
    "\n",
    "encoder_model = Encoder(num_conv_layers=num_conv_layers, \n",
    "                        encoder_conv_filters=encoder_conv_filters, \n",
    "                        encoder_output_dim=encoder_output_dim, \n",
    "                        encoder_strides=encoder_strides,\n",
    "                        input_dim=input_dim)\n",
    "encoder_model.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "northern-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1568)              4704      \n",
      "_________________________________________________________________\n",
      "reshape_26 (Reshape)         (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_0 (Conv2DTransp (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "decoder_conv_1 (Conv2DTransp (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "decoder_conv_2 (Conv2DTransp (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "output_layer (Conv2DTranspos (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 69,953\n",
      "Trainable params: 69,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_conv_layers = 4\n",
    "decoder_conv_filters = [32, 64, 64, 32]\n",
    "encoder_output_dim=2\n",
    "decoder_strides = [1,2,2,1]\n",
    "input_dim=(2,)\n",
    "\n",
    "decoder_model = Decoder(num_conv_layers = num_conv_layers, \n",
    "                        decoder_conv_filters = decoder_conv_filters, \n",
    "                        shape_before_flat = (7, 7, 32), \n",
    "                        decoder_strides = decoder_strides,\n",
    "                        input_dim=input_dim,\n",
    "                        dropout_rate=None, \n",
    "                        name='decoder')\n",
    "decoder_model.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "tribal-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(num_conv_layers = num_conv_layers,\n",
    "                    encoder_conv_filters = encoder_conv_filters,\n",
    "                    encoder_strides = encoder_strides,\n",
    "                    encoder_output_dim = encoder_output_dim, \n",
    "                    input_dim = (28, 28, 1), \n",
    "                    decoder_conv_filters = decoder_conv_filters, \n",
    "                    shape_before_flat = shape_before_flat, \n",
    "                    decoder_strides = decoder_strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "tired-tutorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Start of epoch 1\n",
      "Start of epoch 2\n",
      "Start of epoch 3\n",
      "Start of epoch 4\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(x_train[:1000]):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = model([x_batch_train])\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "effective-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)      (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)      (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "encoder_output (Dense)       (None, 2)                 6274      \n",
      "=================================================================\n",
      "Total params: 117,698\n",
      "Trainable params: 117,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_conv_layers = 4\n",
    "conv_units = 64\n",
    "encoder_output_dim=2\n",
    "strides = [1,2,2,1]\n",
    "input_dim=(28, 28, 1)\n",
    "dropout_rate=0.2\n",
    "\n",
    "encoder_input = Input(shape=input_dim)\n",
    "x = encoder_input\n",
    "for i in range(num_conv_layers):\n",
    "    conv_layer = Conv2D(filters=conv_units,\n",
    "                        kernel_size=(3, 3),\n",
    "                        strides=strides[i],\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        name=f'encoder_conv_{i}')\n",
    "    x = conv_layer(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "shape_before_flat = K.int_shape(x)[1:]\n",
    "x = Flatten()(x)\n",
    "dense_layer = Dense(encoder_output_dim, name='encoder_output')\n",
    "encoder_output = dense_layer(x)\n",
    "\n",
    "encoder_model = Model(encoder_input, encoder_output)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "central-improvement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_before_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "modified-baker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_63 (InputLayer)        [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_0 (Conv2DTransp (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_1 (Conv2DTransp (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_2 (Conv2DTransp (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_final (Conv2DTr (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 120,769\n",
      "Trainable params: 120,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_input = Input(shape=encoder_output_dim)\n",
    "x = Dense(np.prod(shape_before_flat))(decoder_input)\n",
    "x = Reshape(shape_before_flat)(x)\n",
    "\n",
    "for i in range(num_conv_layers - 1):\n",
    "    conv_layer = Conv2DTranspose(filters=conv_units,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 strides=strides[i], \n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 name=f'decoder_conv_{i}')\n",
    "    x = conv_layer(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "output_layer = Conv2DTranspose(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1, \n",
    "                               padding='same',\n",
    "                               activation='sigmoid',\n",
    "                               name=f'decoder_conv_final')\n",
    "x = output_layer(x)\n",
    "decoder_output = x\n",
    "\n",
    "decoder_model = Model(decoder_input, decoder_output)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "incomplete-brake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.1555\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.1096\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.1096\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.1096\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.1096: 0s - los\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.1096: 0s - loss\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1096: 0s \n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.1096\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 2s 63ms/step - loss: 0.1096\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 2s 69ms/step - loss: 0.1096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d80e1c5c70>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = encoder_input\n",
    "model_output = decoder_model(encoder_output)\n",
    "\n",
    "model = Model(encoder_input, model_output)\n",
    "\n",
    "\n",
    "def r_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\n",
    "model.compile(optimizer='adam', loss=r_loss)\n",
    "model.fit(x_train[:1000], x_train[:1000], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-stocks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "greatest-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "established-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "embedded-madison",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-f098ea6a4c20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\my_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;31m# Legacy graph support is contained in `training_v1.Model`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2567\u001b[0m     \u001b[1;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2569\u001b[1;33m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[0;32m   2570\u001b[0m                          \u001b[1;34m'training/testing. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2571\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model.fit(x_train[:1000], x_train[:1000], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "arabic-convert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Encoder.call of <__main__.Encoder object at 0x000002D80E1A45B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Encoder.call of <__main__.Encoder object at 0x000002D80E1A45B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(x_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "guided-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d815336850>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(res[1], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "reserved-riverside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17a88ee1940>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANOklEQVR4nO3db6hc9Z3H8c9n3TSCqZq7uWq0cdPmijaIm5YhrLpUV92QBCH2QZcEKVmQpqBiC0VXXLSKT8JqUwpKNVFpunQtxVQSJLiVUNE8sGQ0UaNh13/XNPWSOzFCUxCyid99cI/LNd45M86Zf8n3/YLLzJzv+fPNkM89c+d3Zn6OCAE49f3VoBsA0B+EHUiCsANJEHYgCcIOJPHX/TzYvHnzYuHChf08JJDK+Pi4Dh065JlqlcJue7mkn0k6TdJjEbG+bP2FCxeqXq9XOSSAErVarWmt45fxtk+T9LCkFZIWS1pje3Gn+wPQW1X+Zl8q6e2IeDcijkr6taRV3WkLQLdVCfsFkv447fGBYtln2F5nu2673mg0KhwOQBVVwj7TmwCfu/Y2IjZGRC0iaqOjoxUOB6CKKmE/IGnBtMdfkfRBtXYA9EqVsO+SdJHtr9r+kqTVkrZ1py0A3dbx0FtEHLN9q6T/0tTQ2xMR8UbXOgPQVZXG2SNiu6TtXeoFQA9xuSyQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiUpTNtsel3RE0nFJxyKi1o2mAHRfpbAX/jEiDnVhPwB6iJfxQBJVwx6Sfmf7ZdvrZlrB9jrbddv1RqNR8XAAOlU17FdGxDclrZB0i+1vnbhCRGyMiFpE1EZHRyseDkCnKoU9Ij4obiclPS1paTeaAtB9HYfd9hm2v/zpfUnLJO3tVmMAuqvKu/HnSnra9qf7+c+IeLYrXQHouo7DHhHvSvq7LvYCoIcYegOSIOxAEoQdSIKwA0kQdiCJbnwQJoWnnnqqaW3Tpk2l255//vml9dNPP720fuONN5bWzzvvvKa1sbGx0m2RB2d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfY23X777U1r4+PjPT32I488Ulo/88wzm9YWL17c7XZOGgsWLGhau+OOO0q3rdVOvS9K5swOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzt6mxx57rGnt1VdfLd221Vj3m2++WVrfvXt3af35559vWnvppZdKt73wwgtL6/v37y+tVzFr1qzS+rx580rrExMTpfWyf3vZGLzEODuAkxhhB5Ig7EAShB1IgrADSRB2IAnCDiTBOHubrr322o5q7Vi+fHml7T/66KOmtVZj9K3Gk3ft2tVRT+2YPXt2af3iiy8urV9yySWl9cOHDzetLVq0qHTbU1HLM7vtJ2xP2t47bdmI7edsv1Xczu1tmwCqaudl/C8knXjquVPSjoi4SNKO4jGAIdYy7BHxgqQTXw+tkrS5uL9Z0g3dbQtAt3X6Bt25ETEhScXtOc1WtL3Odt12vdFodHg4AFX1/N34iNgYEbWIqI2Ojvb6cACa6DTsB23Pl6TidrJ7LQHohU7Dvk3S2uL+Wklbu9MOgF5pOc5u+0lJV0uaZ/uApB9LWi/pN7ZvkrRf0nd62STKzZ3bfOTzmmuuqbTvqtcQVLFly5bSetn1BZJ02WWXNa2tXr26o55OZi3DHhFrmpQG978AwBfG5bJAEoQdSIKwA0kQdiAJwg4kwUdcMTCTk+XXYt18882l9Ygord9zzz1NayMjI6Xbnoo4swNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzY2Aefvjh0nqrcfizzz67tN7qq6iz4cwOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzo6e2rlzZ9Pa+vXrK+1769by6QouvfTSSvs/1XBmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGdHT23fvr1p7ejRo6XbXnfddaX1yy+/vKOesmp5Zrf9hO1J23unLbvX9p9s7yl+Vva2TQBVtfMy/heSls+w/KcRsaT4af7rG8BQaBn2iHhB0uE+9AKgh6q8QXer7deKl/lzm61ke53tuu16o9GocDgAVXQa9p9LWiRpiaQJST9ptmJEbIyIWkTURkdHOzwcgKo6CntEHIyI4xHxiaRNkpZ2ty0A3dZR2G3Pn/bw25L2NlsXwHBoOc5u+0lJV0uaZ/uApB9Lutr2EkkhaVzS93vXIobZxx9/XFp/9tlnm9Zmz55duu19991XWp81a1ZpHZ/VMuwRsWaGxY/3oBcAPcTlskAShB1IgrADSRB2IAnCDiTBR1xRyQMPPFBa3717d9PaihUrSre94oorOuoJM+PMDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6OUs8880xp/f777y+tn3XWWU1rd999d0c9oTOc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZk/vwww9L67fddltp/dixY6X1lSubT/DLlMv9xZkdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnP0Ud/z48dL68uXLS+vvvfdeaX1sbKy03urz7uiflmd22wts/972Pttv2P5BsXzE9nO23ypu5/a+XQCdaudl/DFJP4qIr0v6e0m32F4s6U5JOyLiIkk7iscAhlTLsEfERES8Utw/ImmfpAskrZK0uVhts6QbetQjgC74Qm/Q2V4o6RuS/iDp3IiYkKZ+IUg6p8k262zXbdcbjUbFdgF0qu2w254jaYukH0bEn9vdLiI2RkQtImqjo6Od9AigC9oKu+1Zmgr6ryLit8Xig7bnF/X5kiZ70yKAbmg59Gbbkh6XtC8iNkwrbZO0VtL64nZrTzpEJe+8805pvV6vV9r/hg0bSuuLFi2qtH90Tzvj7FdK+q6k123vKZbdpamQ/8b2TZL2S/pOTzoE0BUtwx4ROyW5Sfna7rYDoFe4XBZIgrADSRB2IAnCDiRB2IEk+IjrKeD9999vWlu2bFmlfT/44IOl9euvv77S/tE/nNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2U8Bjz76aNNa2Rh8O6666qrS+tTXHeBkwJkdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnP0k8OKLL5bWH3rooT51gpMZZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKKd+dkXSPqlpPMkfSJpY0T8zPa9kr4nqVGseldEbO9Vo5nt3LmztH7kyJGO9z02NlZanzNnTsf7xnBp56KaY5J+FBGv2P6ypJdtP1fUfhoR5bMIABgK7czPPiFporh/xPY+SRf0ujEA3fWF/ma3vVDSNyT9oVh0q+3XbD9he26TbdbZrtuuNxqNmVYB0Adth932HElbJP0wIv4s6eeSFklaoqkz/09m2i4iNkZELSJqo6Oj1TsG0JG2wm57lqaC/quI+K0kRcTBiDgeEZ9I2iRpae/aBFBVy7B76utDH5e0LyI2TFs+f9pq35a0t/vtAeiWdt6Nv1LSdyW9bntPsewuSWtsL5EUksYlfb8H/aGiJUuWlNZ37NhRWh8ZGeliNxikdt6N3ylppi8HZ0wdOIlwBR2QBGEHkiDsQBKEHUiCsANJEHYgCUdE3w5Wq9WiXq/37XhANrVaTfV6fcZ5tDmzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASfR1nt92Q9P60RfMkHepbA1/MsPY2rH1J9Napbvb2txEx4/e/9TXsnzu4XY+I2sAaKDGsvQ1rXxK9dapfvfEyHkiCsANJDDrsGwd8/DLD2tuw9iXRW6f60ttA/2YH0D+DPrMD6BPCDiQxkLDbXm77v22/bfvOQfTQjO1x26/b3mN7oB++L+bQm7S9d9qyEdvP2X6ruJ1xjr0B9Xav7T8Vz90e2ysH1NsC27+3vc/2G7Z/UCwf6HNX0ldfnre+/81u+zRJ/yPpnyQdkLRL0pqIeLOvjTRhe1xSLSIGfgGG7W9J+oukX0bEpcWyf5d0OCLWF78o50bEvw5Jb/dK+sugp/EuZiuaP32acUk3SPoXDfC5K+nrn9WH520QZ/alkt6OiHcj4qikX0taNYA+hl5EvCDp8AmLV0naXNzfrKn/LH3XpLehEBETEfFKcf+IpE+nGR/oc1fSV18MIuwXSPrjtMcHNFzzvYek39l+2fa6QTczg3MjYkKa+s8j6ZwB93OiltN499MJ04wPzXPXyfTnVQ0i7DN9P9Ywjf9dGRHflLRC0i3Fy1W0p61pvPtlhmnGh0Kn059XNYiwH5C0YNrjr0j6YAB9zCgiPihuJyU9reGbivrgpzPoFreTA+7n/w3TNN4zTTOuIXjuBjn9+SDCvkvSRba/avtLklZL2jaAPj7H9hnFGyeyfYakZRq+qai3SVpb3F8raesAe/mMYZnGu9k04xrwczfw6c8jou8/klZq6h35dyT92yB6aNLX1yS9Wvy8MejeJD2pqZd1/6upV0Q3SfobSTskvVXcjgxRb/8h6XVJr2kqWPMH1Ns/aOpPw9ck7Sl+Vg76uSvpqy/PG5fLAklwBR2QBGEHkiDsQBKEHUiCsANJEHYgCcIOJPF/gfXs6RJfv5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-substitute",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
