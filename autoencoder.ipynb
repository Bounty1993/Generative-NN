{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Layer, Input, Conv2D, BatchNormalization, Dropout, Dense, \n",
    "                                     Flatten, Reshape, Conv2DTranspose)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "handy-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, num_conv_layers, encoder_conv_filters, encoder_strides,\n",
    "                 encoder_output_dim, dropout_rate=None, input_dim=None, name='encoder', **kwargs):\n",
    "        #super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.conv_layers = []\n",
    "        for i in range(num_conv_layers):\n",
    "            conv_layer = Conv2D(filters=encoder_conv_filters[i],\n",
    "                                kernel_size=(3, 3),\n",
    "                                strides=encoder_strides[i],\n",
    "                                padding='same',\n",
    "                                activation='relu',\n",
    "                                name=f'encoder_conv_{i}')\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            \n",
    "        self.dense_layer = Dense(encoder_output_dim, name='encoder_output')\n",
    "        \n",
    "    \n",
    "    def model(self):\n",
    "        model_input = Input(shape=self.input_dim)\n",
    "        return Model(model_input, self.call(model_input))\n",
    "            \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for conv_layer in self.conv_layers:\n",
    "\n",
    "            x = conv_layer(x)\n",
    "        x = Flatten()(x)\n",
    "        #self.shape_before_flat = K.int_shape(x)[1:] \n",
    "        x = self.dense_layer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Decoder:\n",
    "    def __init__(self, num_conv_layers, decoder_conv_filters, shape_before_flat, \n",
    "                 decoder_strides, dropout_rate=None, input_dim=None, name='decoder', **kwargs):\n",
    "        #super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.dense = Dense(np.prod(shape_before_flat), activation='relu')\n",
    "        self.reshape = Reshape(shape_before_flat)\n",
    "        self.conv_layers = []\n",
    "        for i in range(num_conv_layers - 1):\n",
    "            conv_layer = Conv2DTranspose(filters=decoder_conv_filters[i],\n",
    "                                         kernel_size=(3, 3),\n",
    "                                         strides=decoder_strides[i],\n",
    "                                         padding='same',\n",
    "                                         activation='relu',\n",
    "                                         name=f'decoder_conv_{i}')\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            \n",
    "        self.output_layer = Conv2DTranspose(filters=1,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=1,\n",
    "                                            padding='same',\n",
    "                                            activation='sigmoid', \n",
    "                                            name=f'output_layer')\n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        model_input = Input(shape=self.input_dim)\n",
    "        return Model(model_input, self.call(model_input))\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        x = self.reshape(x)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "welsh-fight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)      (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)      (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "encoder_output (Dense)       (None, 2)                 6274      \n",
      "=================================================================\n",
      "Total params: 117,698\n",
      "Trainable params: 117,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_conv_layers = 4\n",
    "encoder_conv_filters = [64, 64, 64, 64]\n",
    "encoder_output_dim=2\n",
    "encoder_strides = [1,2,2,1]\n",
    "input_dim=(28, 28, 1)\n",
    "\n",
    "encoder_model = Encoder(num_conv_layers=num_conv_layers, \n",
    "                        encoder_conv_filters=encoder_conv_filters, \n",
    "                        encoder_output_dim=encoder_output_dim, \n",
    "                        encoder_strides=encoder_strides,\n",
    "                        input_dim=input_dim)\n",
    "encoder_model.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hydraulic-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_0 (Conv2DTransp (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_1 (Conv2DTransp (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_2 (Conv2DTransp (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "output_layer (Conv2DTranspos (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 120,769\n",
      "Trainable params: 120,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_conv_layers = 4\n",
    "decoder_conv_filters = [64, 64, 64, 64]\n",
    "encoder_output_dim=2\n",
    "decoder_strides = [1,2,2,1]\n",
    "input_dim=(2,)\n",
    "\n",
    "decoder_model = Decoder(num_conv_layers = num_conv_layers, \n",
    "                        decoder_conv_filters = decoder_conv_filters, \n",
    "                        shape_before_flat = (7, 7, 64), \n",
    "                        decoder_strides = decoder_strides,\n",
    "                        input_dim=input_dim,\n",
    "                        dropout_rate=None, \n",
    "                        name='decoder')\n",
    "decoder_model.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ordered-province",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)      (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)      (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "encoder_output (Dense)       (None, 2)                 6274      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_0 (Conv2DTransp (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_1 (Conv2DTransp (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_2 (Conv2DTransp (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "output_layer (Conv2DTranspos (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 238,467\n",
      "Trainable params: 238,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=(28, 28, 1))\n",
    "encoder_output = encoder_model.call(encoder_input)\n",
    "decoder_output = decoder_model.call(encoder_output)\n",
    "\n",
    "model = Model(encoder_input, decoder_output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "effective-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_conv_layers = 4\n",
    "conv_units = 64\n",
    "encoder_output_dim=2\n",
    "strides = [1,2,2,1]\n",
    "input_dim=(28, 28, 1)\n",
    "dropout_rate=0.2\n",
    "\n",
    "encoder_input = Input(shape=input_dim)\n",
    "x = encoder_input\n",
    "for i in range(num_conv_layers):\n",
    "    conv_layer = Conv2D(filters=conv_units,\n",
    "                        kernel_size=(3, 3),\n",
    "                        strides=strides[i],\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        name=f'encoder_conv_{i}')\n",
    "    x = conv_layer(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "shape_before_flat = K.int_shape(x)[1:]\n",
    "x = Flatten()(x)\n",
    "dense_layer = Dense(encoder_output_dim, name='encoder_output')\n",
    "encoder_output = dense_layer(x)\n",
    "\n",
    "#encoder_model = Model(encoder_input, encoder_output)\n",
    "#encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "stretch-brick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_before_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "modified-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=encoder_output_dim)\n",
    "x = Dense(np.prod(shape_before_flat))(decoder_input)\n",
    "x = Reshape(shape_before_flat)(x)\n",
    "\n",
    "for i in range(num_conv_layers - 1):\n",
    "    conv_layer = Conv2DTranspose(filters=conv_units,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 strides=strides[i], \n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 name=f'decoder_conv_{i}')\n",
    "    x = conv_layer(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "output_layer = Conv2DTranspose(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1, \n",
    "                               padding='same',\n",
    "                               activation='sigmoid',\n",
    "                               name=f'decoder_conv_final')\n",
    "x = output_layer(x)\n",
    "decoder_output = x\n",
    "\n",
    "#decoder_model = Model(decoder_input, decoder_output)\n",
    "\n",
    "#decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "incomplete-brake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.1473\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0914\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.0682\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 0.0665\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0649\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 0.0617\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0592\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0572\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0559\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.0546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f519db90d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = encoder_input\n",
    "model_output = decoder_model(encoder_output)\n",
    "\n",
    "model = Model(encoder_input, model_output)\n",
    "\n",
    "\n",
    "def r_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\n",
    "model.compile(optimizer='adam', loss=r_loss)\n",
    "model.fit(x_train[:1000], x_train[:1000], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-stocks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "greatest-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "established-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "embedded-madison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0542\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0530\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 3s 91ms/step - loss: 0.0522\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 3s 98ms/step - loss: 0.0518\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0511\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0506\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0499\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0495\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0489\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f51b306070>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train[:1000], x_train[:1000], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "arabic-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(x_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "guided-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f51b4e87c0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(res[2], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "reserved-riverside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17a88ee1940>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-substitute",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
